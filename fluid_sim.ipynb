{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scnerd\\Anaconda3\\envs\\Anaconda34\\lib\\site-packages\\theano\\configdefaults.py:1695: UserWarning: Theano does not recognise this flag: Value\n",
      "  warnings.warn('Theano does not recognise this flag: {0}'.format(key))\n",
      "DEBUG: nvcc STDOUT mod.cu\n",
      "   Creating library C:/Users/scnerd/AppData/Local/Theano/compiledir_Windows-10-10.0.14931-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.4.5-64/tmpetptz2j6/m91973e5c136ea49268a916ff971b7377.lib and object C:/Users/scnerd/AppData/Local/Theano/compiledir_Windows-10-10.0.14931-Intel64_Family_6_Model_94_Stepping_3_GenuineIntel-3.4.5-64/tmpetptz2j6/m91973e5c136ea49268a916ff971b7377.exp\n",
      "\n",
      "Using gpu device 0: GeForce GTX 980 Ti (CNMeM is disabled, cuDNN 5105)\n",
      "C:\\Users\\scnerd\\Anaconda3\\envs\\Anaconda34\\lib\\site-packages\\theano\\sandbox\\cuda\\__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "from pycuda.compiler import SourceModule\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.signal.conv\n",
    "import theano.misc.pycuda_init\n",
    "import theano.sandbox.cuda as cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class PyCUDAHist2D(theano.Op):\n",
    "    __props__ = ('xres', 'yres', 'xmin', 'xptp', 'ymin', 'yptp', 'length', 'num_blocks', 'threads')\n",
    "\n",
    "    def __init__(self, width, height, xmin=-1.0, xmax=1.0, ymin=-1.0, ymax=1.0, length=16, num_blocks=4, threads=2**10):\n",
    "        self.xres = width\n",
    "        self.yres = height\n",
    "        self.xmin = xmin\n",
    "        self.xptp = xmax - xmin\n",
    "        self.ymin = ymin\n",
    "        self.yptp = ymax - ymin\n",
    "        self.length = length\n",
    "        self.num_blocks = num_blocks\n",
    "        self.threads = threads\n",
    "\n",
    "    def make_node(self, x, y, w):\n",
    "        x = cuda.basic_ops.gpu_contiguous(cuda.basic_ops.as_cuda_ndarray_variable(x))\n",
    "        y = cuda.basic_ops.gpu_contiguous(cuda.basic_ops.as_cuda_ndarray_variable(y))\n",
    "        w = cuda.basic_ops.gpu_contiguous(cuda.basic_ops.as_cuda_ndarray_variable(w))\n",
    "        out_t = cuda.CudaNdarrayType((False, False), dtype='float32')()\n",
    "        out_x = cuda.CudaNdarrayType((False,))()\n",
    "        out_y = cuda.CudaNdarrayType((False,))()\n",
    "        assert x.dtype == 'float32'\n",
    "        assert y.dtype == 'float32'\n",
    "        assert w.dtype == 'float32'\n",
    "        return theano.Apply(self, [x, y, w], [out_t, out_x, out_y])\n",
    "\n",
    "    # Based on\n",
    "    #https://devblogs.nvidia.com/parallelforall/gpu-pro-tip-fast-histograms-using-shared-atomics-maxwell/\n",
    "    def make_thunk(self, node, storage_map, _, _2):\n",
    "\n",
    "        # Creates a histogram in BLOCKS_X pieces, which then need to be added together\n",
    "        code = [\n",
    "        '''\n",
    "        #define SZ ({xres} * {yres})\n",
    "        \n",
    "        __global__ void histogram2d(const float *in_x, const float *in_y, const float *in_w, int n, float *out, float *out_binx, float *out_biny) {{\n",
    "            int start = threadIdx.x + {length} * blockDim.x * (blockIdx.y * gridDim.x + blockIdx.x);\n",
    "                \n",
    "            __shared__ float block_out[SZ];\n",
    "            for(int i = threadIdx.x; i < SZ; i += blockDim.x) {{\n",
    "                block_out[i] = (float)0.0;\n",
    "            }}\n",
    "\n",
    "            __syncthreads();\n",
    "            for(int i = 0; i < {length} && start + i * {length} < n; i++) {{\n",
    "                int pos = start + i * {length};\n",
    "                float x = in_x[pos];\n",
    "                float y = in_y[pos];\n",
    "                int xbin = (int) (((x - {xmin}) / {xptp}) * {xres});\n",
    "                int ybin = (int) (((y - {ymin}) / {yptp}) * {yres});\n",
    "\n",
    "                if (0 <= xbin && xbin < {xres} && 0 <= ybin && ybin < {yres}) {{\n",
    "                    out_binx[pos] = (float)xbin;\n",
    "                    out_biny[pos] = (float)ybin;\n",
    "                    atomicAdd(&out[ybin * {xres} + xbin], in_w[pos]);\n",
    "                }}\n",
    "            }}\n",
    "            \n",
    "            __syncthreads();\n",
    "            for(int i = threadIdx.x; i < SZ; i += blockDim.x) {{\n",
    "                atomicAdd(&out[SZ * blockIdx.x + i], block_out[i]);\n",
    "            }}\n",
    "        }}\n",
    "        ''',\n",
    "        '''\n",
    "        __global__ void histogram2d(const float *in_x, const float *in_y, const float *in_w, int n, float *out, float *out_binx, float *out_biny) {{\n",
    "            int start = threadIdx.x + {length} * blockDim.x * (blockIdx.y * gridDim.x + blockIdx.x);\n",
    "\n",
    "            for(int i = 0; i < {length} && start + i * {length} < n; i++) {{\n",
    "                int pos = start + i * {length};\n",
    "                float x = in_x[pos];\n",
    "                float y = in_y[pos];\n",
    "                int xbin = (int) (((x - {xmin}) / {xptp}) * {xres});\n",
    "                int ybin = (int) (((y - {ymin}) / {yptp}) * {yres});\n",
    "\n",
    "                if (0 <= xbin && xbin < {xres} && 0 <= ybin && ybin < {yres}) {{\n",
    "                    out_binx[pos] = (float)xbin;\n",
    "                    out_biny[pos] = (float)ybin;\n",
    "                    atomicAdd(&out[ybin * {xres} + xbin], in_w[pos]);\n",
    "                }}\n",
    "            }}\n",
    "        }}\n",
    "        ''']\n",
    "        code = code[1].format(**{p: eval(\"self.{}\".format(p), {'self': self}) for p in self.__props__})\n",
    "        mod = SourceModule(code)\n",
    "        cuda_hist = mod.get_function('histogram2d')\n",
    "\n",
    "        inputs = [storage_map[v] for v in node.inputs]\n",
    "        outputs = [storage_map[v] for v in node.outputs]\n",
    "\n",
    "        _x, _y, _w = inputs\n",
    "        (_out,_outx,_outy) = outputs\n",
    "\n",
    "        def run_hist():\n",
    "            x = _x[0]\n",
    "            y = _y[0]\n",
    "            w = _w[0]\n",
    "            n = x.size\n",
    "            xres = self.xres\n",
    "            yres = self.yres\n",
    "            length = self.length\n",
    "            threads_per_block = self.threads\n",
    "            num_blocks = min(self.num_blocks, int(np.ceil(n / threads_per_block / length)))\n",
    "            num_chunks = int(np.ceil(n / num_blocks / threads_per_block / length))\n",
    "            print(num_chunks, num_blocks, threads_per_block, length)\n",
    "            if _out[0] is None or _out[0].shape != (yres, xres):\n",
    "                _out[0] = cuda.CudaNdarray.zeros((yres, xres))\n",
    "                _outx[0] = cuda.CudaNdarray.zeros((n,))\n",
    "                _outy[0] = cuda.CudaNdarray.zeros((n,))\n",
    "            cuda_hist(x, y, w, numpy.int32(n), _out[0], _outx[0], _outy[0], block=(threads_per_block,1,1), grid=(num_blocks, num_chunks))\n",
    "\n",
    "        return run_hist#, zeros((num_blocks, yres, xres, num_chans), 'float32')\n",
    "    \n",
    "def test():\n",
    "    n = 100000000\n",
    "    x = np.random.randn(n)\n",
    "    y = np.random.randn(n)\n",
    "    w = np.ones(n)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.hist2d(x, y, bins=[np.linspace(-10, 10, 101), np.linspace(-10, 10, 101)])\n",
    "    plt.show(block=False)\n",
    "\n",
    "    _x = T.fvector('x')\n",
    "    _y = T.fvector('y')\n",
    "    _w = T.fvector('w')\n",
    "    hister = PyCUDAHist2D(100, 100, xmin=-10, xmax=10, ymin=-10, ymax=10, length=1024)\n",
    "    f = theano.function([_x, _y, _w], hister(_x, _y, _w), allow_input_downcast=True) \n",
    "\n",
    "    from time import time\n",
    "    start = time()\n",
    "    hist, x_assgn, y_assgn = f(x, y, w)\n",
    "    print(time() - start)\n",
    "    hist = np.array(hist)\n",
    "    x_assgn = np.cast['int'](np.array(x_assgn))\n",
    "    y_assgn = np.cast['int'](np.array(y_assgn))\n",
    "    print(hist.shape)\n",
    "    plt.figure()\n",
    "    plt.imshow(hist)\n",
    "    plt.show(block=False)\n",
    "    print(list(zip(x[:10], y[:10], x_assgn[:10], y_assgn[:10])))\n",
    "    print(list(zip(x[-10:], y[-10:], x_assgn[-10:], y_assgn[-10:])))\n",
    "    print(sum(1 for x, y in zip(x_assgn, y_assgn) if x == y and y == 0))\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class PyCUDASlopeKernel(theano.Op):\n",
    "    __props__ = ('height', 'width')\n",
    "\n",
    "    def __init__(self, height, width):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def make_node(self, h):\n",
    "        h = cuda.basic_ops.gpu_contiguous(cuda.basic_ops.as_cuda_ndarray_variable(h))\n",
    "        out_dx = cuda.CudaNdarrayType((False, False), dtype='float32')()\n",
    "        out_dy = cuda.CudaNdarrayType((False, False), dtype='float32')()\n",
    "        assert h.dtype == 'float32'\n",
    "        return theano.Apply(self, [h], [out_dx, out_dy])\n",
    "\n",
    "    # Based on\n",
    "    #https://devblogs.nvidia.com/parallelforall/gpu-pro-tip-fast-histograms-using-shared-atomics-maxwell/\n",
    "    def make_thunk(self, node, storage_map, _, _2):\n",
    "\n",
    "        # Creates a histogram in BLOCKS_X pieces, which then need to be added together\n",
    "        code = '''\n",
    "        __device__ __inline__ float val_at(const float *hist, int x, int y) {{\n",
    "            x = min(max(x, 0), {width} - 1);\n",
    "            y = min(max(y, 0), {height} - 1);\n",
    "            return hist[y * {width} + x];\n",
    "        }}\n",
    "        \n",
    "        __device__ __inline__ float clipper(float mid, const float *hist, int x, int y) {{\n",
    "            // return min(mid, val_at(hist, x, y));\n",
    "            return val_at(hist, x, y);\n",
    "        }}\n",
    "\n",
    "        __global__ void slope_kernel(const float *histogram, float *dx, float *dy) {{\n",
    "            int x = (blockIdx.x * blockDim.x + threadIdx.x);\n",
    "            int y = (blockIdx.y * blockDim.y + threadIdx.y);\n",
    "            if (x < {width} && y < {height}) {{\n",
    "                float v4 = val_at(histogram, x, y);\n",
    "                float d0 = v4 - clipper(v4, histogram, x-1, y-1);\n",
    "                float d1 = v4 - clipper(v4, histogram, x,   y-1);\n",
    "                float d2 = v4 - clipper(v4, histogram, x+1, y-1);\n",
    "                float d3 = v4 - clipper(v4, histogram, x-1, y  );\n",
    "                float d5 = v4 - clipper(v4, histogram, x+1, y  );\n",
    "                float d6 = v4 - clipper(v4, histogram, x-1, y+1);\n",
    "                float d7 = v4 - clipper(v4, histogram, x,   y+1);\n",
    "                float d8 = v4 - clipper(v4, histogram, x+1, y+1);\n",
    "\n",
    "                dx[y * {width} + x] = {root2} * (d2 + d8 - d0 - d6) + d5 - d3;\n",
    "                dy[y * {width} + x] = {root2} * (d6 + d8 - d0 - d2) + d7 - d1;\n",
    "            }}\n",
    "        }}\n",
    "        '''.format(height=self.height, width=self.width, root2=np.sqrt(2))\n",
    "        mod = SourceModule(code)\n",
    "        cuda_kern = mod.get_function('slope_kernel')\n",
    "\n",
    "        inputs = [storage_map[v] for v in node.inputs]\n",
    "        outputs = [storage_map[v] for v in node.outputs]\n",
    "\n",
    "        (_h,) = inputs\n",
    "        (_outx,_outy) = outputs\n",
    "\n",
    "        threads = 16\n",
    "        num_blocks_h = int(np.ceil(self.height / threads))\n",
    "        num_blocks_w = int(np.ceil(self.width / threads))\n",
    "        def run_kern():\n",
    "            if _outx[0] is None or _outx[0].shape != (self.height, self.width):\n",
    "                _outx[0] = cuda.CudaNdarray.zeros((self.height, self.width))\n",
    "                _outy[0] = cuda.CudaNdarray.zeros((self.height, self.width))\n",
    "            cuda_kern(_h[0], _outx[0], _outy[0], block=(threads,threads,1), grid=(num_blocks_w,num_blocks_h))\n",
    "\n",
    "        return run_kern\n",
    "    \n",
    "\n",
    "def sloper(hist):\n",
    "    r2 = np.cast['float32'](np.sqrt(2)/2)\n",
    "    dx_kern = np.array([[-r2, 0, r2],[-1, 0, 1],[-r2, 0, r2]])\n",
    "    dy_kern = np.array([[-r2, -1, -r2],[0, 0, 0],[r2, 1, r2]])\n",
    "    final_kern = np.cast['float32'](np.stack([dx_kern, dy_kern], axis=0))\n",
    "    res = T.signal.conv.conv2d(hist, final_kern, border_mode='full')\n",
    "    return res[0, 1:-1, 1:-1], res[1, 1:-1, 1:-1]\n",
    "    \n",
    "def test():\n",
    "    x = np.arange(100)\n",
    "    y = np.arange(100)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    g = np.exp(-( (x - 50)**2 / 50 + (y - 50)**2 / 50 ))\n",
    "    plt.figure()\n",
    "    plt.imshow(g)\n",
    "    plt.show(block=False)\n",
    "\n",
    "    _hist = T.fmatrix('hist')\n",
    "    #sloper = PyCUDASlopeKernel(100, 100)\n",
    "    f = theano.function([_hist], sloper(_hist), allow_input_downcast=True)\n",
    "\n",
    "    from time import time\n",
    "    start = time()\n",
    "    dx, dy = f(g)\n",
    "    print(time() - start)\n",
    "    print(dx.shape)\n",
    "    print(dy.shape)\n",
    "    plt.figure()\n",
    "    plt.imshow(dx)\n",
    "    plt.show(block=False)\n",
    "    plt.figure()\n",
    "    plt.imshow(dy)\n",
    "    plt.show(block=False)\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scnerd\\Anaconda3\\envs\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:13: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\scnerd\\Anaconda3\\envs\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:15: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GpuElemwise{Composite{Switch(i0, (i1 - i2), Switch(i3, (-i2), i2))}}[(0, 2)] [id A] ''   48\n",
      " |GpuFromHost [id B] ''   40\n",
      " | |Elemwise{Cast{float32}} [id C] ''   29\n",
      " |   |Elemwise{gt,no_inplace} [id D] ''   23\n",
      " |     |HostFromGpu [id E] ''   15\n",
      " |     | |GpuElemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)] [id F] ''   8\n",
      " |     |   |posx [id G]\n",
      " |     |   |GpuDimShuffle{x} [id H] ''   4\n",
      " |     |   | |<CudaNdarrayType(float32, scalar)> [id I]\n",
      " |     |   |velx [id J]\n",
      " |     |TensorConstant{(1,) of 1} [id K]\n",
      " |CudaNdarrayConstant{[ 2.]} [id L]\n",
      " |GpuElemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)] [id F] ''   8\n",
      " |GpuFromHost [id M] ''   42\n",
      "   |Elemwise{Cast{float32}} [id N] ''   31\n",
      "     |Elemwise{lt,no_inplace} [id O] ''   24\n",
      "       |HostFromGpu [id E] ''   15\n",
      "       |TensorConstant{(1,) of 0} [id P]\n",
      "GpuElemwise{Composite{Switch(i0, (i1 * Composite{(i0 + ((i1 * i2) / i3))}(i2, i3, i4, i5)), Composite{(i0 + ((i1 * i2) / i3))}(i2, i3, i4, i5))}}[(0, 2)] [id Q] ''   74\n",
      " |GpuFromHost [id R] ''   41\n",
      " | |Elemwise{Composite{Cast{float32}(OR(i0, i1))}} [id S] ''   30\n",
      " |   |Elemwise{lt,no_inplace} [id O] ''   24\n",
      " |   |Elemwise{gt,no_inplace} [id D] ''   23\n",
      " |CudaNdarrayConstant{[-0.89999998]} [id T]\n",
      " |velx [id J]\n",
      " |GpuDimShuffle{x} [id H] ''   4\n",
      " |GpuFromHost [id U] ''   72\n",
      " | |AdvancedSubtensor [id V] ''   70\n",
      " |   |HostFromGpu [id W] ''   68\n",
      " |   | |GpuSubtensor{int64, int64:int64:, int64:int64:} [id X] ''   66\n",
      " |   |   |GpuDimShuffle{2,1,0} [id Y] ''   64\n",
      " |   |   | |GpuReshape{3} [id Z] ''   63\n",
      " |   |   |   |GpuDimShuffle{3,2,1,0} [id BA] ''   62\n",
      " |   |   |   | |GpuDnnConv{algo='small', inplace=True} [id BB] ''   61\n",
      " |   |   |   |   |GpuContiguous [id BC] ''   46\n",
      " |   |   |   |   | |GpuReshape{4} [id BD] ''   36\n",
      " |   |   |   |   |   |GpuElemwise{Sqr}[(0, 0)] [id BE] ''   14\n",
      " |   |   |   |   |   | |PyCUDAHist2D{xres=320, yres=240, xmin=0, xptp=1, ymin=0, yptp=1, length=1024, num_blocks=4, threads=1024}.0 [id BF] ''   5\n",
      " |   |   |   |   |   |   |GpuContiguous [id BG] ''   3\n",
      " |   |   |   |   |   |   | |posx [id G]\n",
      " |   |   |   |   |   |   |GpuContiguous [id BH] ''   0\n",
      " |   |   |   |   |   |   | |posy [id BI]\n",
      " |   |   |   |   |   |   |GpuContiguous [id BJ] ''   1\n",
      " |   |   |   |   |   |     |sizes [id BK]\n",
      " |   |   |   |   |   |Join [id BL] ''   25\n",
      " |   |   |   |   |     |TensorConstant{0} [id BM]\n",
      " |   |   |   |   |     |TensorConstant{(2,) of 1} [id BN]\n",
      " |   |   |   |   |     |MakeVector{dtype='int64'} [id BO] ''   18\n",
      " |   |   |   |   |       |Shape_i{0} [id BP] ''   12\n",
      " |   |   |   |   |       | |PyCUDAHist2D{xres=320, yres=240, xmin=0, xptp=1, ymin=0, yptp=1, length=1024, num_blocks=4, threads=1024}.0 [id BF] ''   5\n",
      " |   |   |   |   |       |Shape_i{1} [id BQ] ''   11\n",
      " |   |   |   |   |         |PyCUDAHist2D{xres=320, yres=240, xmin=0, xptp=1, ymin=0, yptp=1, length=1024, num_blocks=4, threads=1024}.0 [id BF] ''   5\n",
      " |   |   |   |   |CudaNdarrayConstant{[[[[-0.70710677  0.          0.70710677]\n",
      "   [-1.          0.          1.        ]\n",
      "   [-0.70710677  0.          0.70710677]]]\n",
      "\n",
      "\n",
      " [[[-0.70710677 -1.         -0.70710677]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.70710677  1.          0.70710677]]]]} [id BR]\n",
      " |   |   |   |   |GpuAllocEmpty [id BS] ''   60\n",
      " |   |   |   |   | |TensorConstant{1} [id BT]\n",
      " |   |   |   |   | |TensorConstant{2} [id BU]\n",
      " |   |   |   |   | |Elemwise{Composite{((((i0 + i1) - i2) // i3) + i3)}}[(0, 0)] [id BV] ''   58\n",
      " |   |   |   |   | | |Elemwise{switch,no_inplace} [id BW] ''   52\n",
      " |   |   |   |   | | | |Elemwise{eq,no_inplace} [id BX] ''   44\n",
      " |   |   |   |   | | | | |Subtensor{int64} [id BY] ''   33\n",
      " |   |   |   |   | | | | | |Join [id BL] ''   25\n",
      " |   |   |   |   | | | | | |Constant{2} [id BZ]\n",
      " |   |   |   |   | | | | |TensorConstant{-1} [id CA]\n",
      " |   |   |   |   | | | |Elemwise{Composite{((i0 * i1) // i2)}}[(0, 0)] [id CB] ''   50\n",
      " |   |   |   |   | | | | |Shape_i{0} [id CC] ''   22\n",
      " |   |   |   |   | | | | | |GpuElemwise{Sqr}[(0, 0)] [id BE] ''   14\n",
      " |   |   |   |   | | | | |Shape_i{1} [id CD] ''   21\n",
      " |   |   |   |   | | | | | |GpuElemwise{Sqr}[(0, 0)] [id BE] ''   14\n",
      " |   |   |   |   | | | | |Elemwise{Composite{(-(i0 * i1 * i2 * i3))}} [id CE] ''   45\n",
      " |   |   |   |   | | | |   |Subtensor{int64} [id CF] ''   35\n",
      " |   |   |   |   | | | |   | |Join [id BL] ''   25\n",
      " |   |   |   |   | | | |   | |Constant{0} [id CG]\n",
      " |   |   |   |   | | | |   |Subtensor{int64} [id CH] ''   34\n",
      " |   |   |   |   | | | |   | |Join [id BL] ''   25\n",
      " |   |   |   |   | | | |   | |Constant{1} [id CI]\n",
      " |   |   |   |   | | | |   |Subtensor{int64} [id BY] ''   33\n",
      " |   |   |   |   | | | |   |Subtensor{int64} [id CJ] ''   32\n",
      " |   |   |   |   | | | |     |Join [id BL] ''   25\n",
      " |   |   |   |   | | | |     |Constant{3} [id CK]\n",
      " |   |   |   |   | | | |Subtensor{int64} [id BY] ''   33\n",
      " |   |   |   |   | | |TensorConstant{4} [id CL]\n",
      " |   |   |   |   | | |TensorConstant{3} [id CM]\n",
      " |   |   |   |   | | |TensorConstant{1} [id CN]\n",
      " |   |   |   |   | |Elemwise{Composite{((((i0 + i1) - i2) // i3) + i3)}}[(0, 0)] [id CO] ''   57\n",
      " |   |   |   |   |   |Elemwise{Switch}[(0, 1)] [id CP] ''   54\n",
      " |   |   |   |   |   | |Elemwise{eq,no_inplace} [id CQ] ''   43\n",
      " |   |   |   |   |   | | |Subtensor{int64} [id CJ] ''   32\n",
      " |   |   |   |   |   | | |TensorConstant{-1} [id CA]\n",
      " |   |   |   |   |   | |Elemwise{Composite{((i0 * i1) // i2)}}[(0, 0)] [id CB] ''   50\n",
      " |   |   |   |   |   | |Subtensor{int64} [id CJ] ''   32\n",
      " |   |   |   |   |   |TensorConstant{4} [id CL]\n",
      " |   |   |   |   |   |TensorConstant{3} [id CR]\n",
      " |   |   |   |   |   |TensorConstant{1} [id CN]\n",
      " |   |   |   |   |GpuDnnConvDesc{border_mode='full', subsample=(1, 1), conv_mode='conv', precision='float32'} [id CS] ''   59\n",
      " |   |   |   |   | |MakeVector{dtype='int64'} [id CT] ''   56\n",
      " |   |   |   |   | | |TensorConstant{1} [id BT]\n",
      " |   |   |   |   | | |TensorConstant{1} [id BT]\n",
      " |   |   |   |   | | |Elemwise{switch,no_inplace} [id BW] ''   52\n",
      " |   |   |   |   | | |Elemwise{Switch}[(0, 1)] [id CP] ''   54\n",
      " |   |   |   |   | |TensorConstant{[2 1 3 3]} [id CU]\n",
      " |   |   |   |   |Constant{1.0} [id CV]\n",
      " |   |   |   |   |Constant{0.0} [id CW]\n",
      " |   |   |   |MakeVector{dtype='int64'} [id CX] ''   55\n",
      " |   |   |     |Elemwise{Composite{(i0 + Switch(i1, i2, i3))}} [id CY] ''   51\n",
      " |   |   |     | |TensorConstant{2} [id BU]\n",
      " |   |   |     | |Elemwise{eq,no_inplace} [id CQ] ''   43\n",
      " |   |   |     | |Elemwise{Composite{((i0 * i1) // i2)}}[(0, 0)] [id CZ] ''   49\n",
      " |   |   |     | | |Shape_i{0} [id BP] ''   12\n",
      " |   |   |     | | |Shape_i{1} [id BQ] ''   11\n",
      " |   |   |     | | |Elemwise{Composite{(-(i0 * i1 * i2 * i3))}} [id CE] ''   45\n",
      " |   |   |     | |Subtensor{int64} [id CJ] ''   32\n",
      " |   |   |     |Elemwise{Composite{(i0 + Switch(i1, i2, i3))}}[(0, 2)] [id DA] ''   53\n",
      " |   |   |     | |TensorConstant{2} [id BU]\n",
      " |   |   |     | |Elemwise{eq,no_inplace} [id BX] ''   44\n",
      " |   |   |     | |Elemwise{Composite{((i0 * i1) // i2)}}[(0, 0)] [id CZ] ''   49\n",
      " |   |   |     | |Subtensor{int64} [id BY] ''   33\n",
      " |   |   |     |TensorConstant{-1} [id DB]\n",
      " |   |   |Constant{0} [id CG]\n",
      " |   |   |Constant{1} [id CI]\n",
      " |   |   |Constant{-1} [id DC]\n",
      " |   |   |Constant{1} [id CI]\n",
      " |   |   |Constant{-1} [id DC]\n",
      " |   |Elemwise{Cast{int32}} [id DD] ''   17\n",
      " |   | |HostFromGpu [id DE] ''   10\n",
      " |   |   |PyCUDAHist2D{xres=320, yres=240, xmin=0, xptp=1, ymin=0, yptp=1, length=1024, num_blocks=4, threads=1024}.2 [id BF] ''   5\n",
      " |   |Elemwise{Cast{int32}} [id DF] ''   16\n",
      " |     |HostFromGpu [id DG] ''   9\n",
      " |       |PyCUDAHist2D{xres=320, yres=240, xmin=0, xptp=1, ymin=0, yptp=1, length=1024, num_blocks=4, threads=1024}.1 [id BF] ''   5\n",
      " |GpuElemwise{sqr,no_inplace} [id DH] ''   2\n",
      "   |sizes [id BK]\n",
      "GpuElemwise{Composite{Switch(i0, (i1 - i2), Switch(i3, (-i2), i2))}}[(0, 2)] [id DI] ''   47\n",
      " |GpuFromHost [id DJ] ''   37\n",
      " | |Elemwise{Cast{float32}} [id DK] ''   26\n",
      " |   |Elemwise{gt,no_inplace} [id DL] ''   19\n",
      " |     |HostFromGpu [id DM] ''   13\n",
      " |     | |GpuElemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)] [id DN] ''   7\n",
      " |     |   |posy [id BI]\n",
      " |     |   |GpuDimShuffle{x} [id H] ''   4\n",
      " |     |   |vely [id DO]\n",
      " |     |TensorConstant{(1,) of 1} [id K]\n",
      " |CudaNdarrayConstant{[ 2.]} [id L]\n",
      " |GpuElemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)] [id DN] ''   7\n",
      " |GpuFromHost [id DP] ''   39\n",
      "   |Elemwise{Cast{float32}} [id DQ] ''   28\n",
      "     |Elemwise{lt,no_inplace} [id DR] ''   20\n",
      "       |HostFromGpu [id DM] ''   13\n",
      "       |TensorConstant{(1,) of 0} [id P]\n",
      "GpuElemwise{Composite{Switch(i0, (i1 * Composite{((i0 + ((i1 * i2) / i3)) + i4)}(i2, i3, i4, i5, i6)), Composite{((i0 + ((i1 * i2) / i3)) + i4)}(i2, i3, i4, i5, i6))}}[(0, 2)] [id DS] ''   73\n",
      " |GpuFromHost [id DT] ''   38\n",
      " | |Elemwise{Composite{Cast{float32}(OR(i0, i1))}} [id DU] ''   27\n",
      " |   |Elemwise{lt,no_inplace} [id DR] ''   20\n",
      " |   |Elemwise{gt,no_inplace} [id DL] ''   19\n",
      " |CudaNdarrayConstant{[-0.89999998]} [id T]\n",
      " |vely [id DO]\n",
      " |GpuDimShuffle{x} [id H] ''   4\n",
      " |GpuFromHost [id DV] ''   71\n",
      " | |AdvancedSubtensor [id DW] ''   69\n",
      " |   |HostFromGpu [id DX] ''   67\n",
      " |   | |GpuSubtensor{int64, int64:int64:, int64:int64:} [id DY] ''   65\n",
      " |   |   |GpuDimShuffle{2,1,0} [id Y] ''   64\n",
      " |   |   |Constant{1} [id CI]\n",
      " |   |   |Constant{1} [id CI]\n",
      " |   |   |Constant{-1} [id DC]\n",
      " |   |   |Constant{1} [id CI]\n",
      " |   |   |Constant{-1} [id DC]\n",
      " |   |Elemwise{Cast{int32}} [id DD] ''   17\n",
      " |   |Elemwise{Cast{int32}} [id DF] ''   16\n",
      " |GpuElemwise{sqr,no_inplace} [id DH] ''   2\n",
      " |GpuElemwise{mul,no_inplace} [id DZ] ''   6\n",
      "   |CudaNdarrayConstant{[ 1000.]} [id EA]\n",
      "   |GpuDimShuffle{x} [id H] ''   4\n"
     ]
    }
   ],
   "source": [
    "cast = np.cast['float32']\n",
    "    \n",
    "height = 240\n",
    "width = 320\n",
    "n = 10000000\n",
    "rebound = cast(0.9)\n",
    "time_step = cast(0.0001)\n",
    "time_step = theano.shared(time_step)\n",
    "hist_length = 2**10\n",
    "gravity = cast((0, 1000))\n",
    "\n",
    "sizes = np.ones(n)\n",
    "_reset_posx = lambda: np.clip(np.hstack([np.random.normal(loc=0.1, scale=0.01, size=n/2), np.random.normal(loc=0.7, scale=0.1, size=n/2)]), 0, 1)\n",
    "_reset_posy = lambda: np.clip(np.random.normal(loc=0.5, scale=0.1, size=n), 0, 1)\n",
    "_reset_velx = lambda: np.hstack([np.random.normal(loc=-10, scale=0.01, size=n/2), np.random.normal(loc=-10, scale=0.01, size=n/2)])\n",
    "_reset_vely = lambda: np.random.normal(loc=0, scale=0.01, size=n)\n",
    "cuda_cast = lambda v: cuda.CudaNdarray(cast(v))\n",
    "\n",
    "_posx = theano.shared(cuda_cast(_reset_posx()), name='posx')\n",
    "_posy = theano.shared(cuda_cast(_reset_posy()), name='posy')\n",
    "_velx = theano.shared(cuda_cast(_reset_velx()), name='velx')\n",
    "_vely = theano.shared(cuda_cast(_reset_vely()), name='vely')\n",
    "_size = theano.shared(cuda_cast(sizes), name='sizes')\n",
    "\n",
    "_mass = _size ** 2 # we're working in 2D, so this is close enough\n",
    "\n",
    "hister = PyCUDAHist2D(width, height, xmin=0, xmax=1, ymin=0, ymax=1, length=hist_length)\n",
    "#sloper = PyCUDASlopeKernel(height, width)\n",
    "\n",
    "hist, x_assgns, y_assgns = hister(_posx, _posy, _size)\n",
    "x_assgns = T.cast(x_assgns, 'int32')\n",
    "y_assgns = T.cast(y_assgns, 'int32')\n",
    "\n",
    "dx, dy = sloper(hist**2)\n",
    "\n",
    "dx = dx[y_assgns, x_assgns]\n",
    "dy = dy[y_assgns, x_assgns]\n",
    "out_velx = _velx + time_step * dx / _mass + time_step * gravity[0]\n",
    "out_vely = _vely + time_step * dy / _mass + time_step * gravity[1]\n",
    "out_posx = _posx + time_step * _velx\n",
    "out_posy = _posy + time_step * _vely\n",
    "\n",
    "off_left = T.lt(out_posx, 0)\n",
    "off_right = T.gt(out_posx, 1)\n",
    "off_top = T.lt(out_posy, 0)\n",
    "off_bott = T.gt(out_posy, 1)\n",
    "rebound_x = off_left | off_right\n",
    "rebound_y = off_top | off_bott\n",
    "\n",
    "out_velx = T.switch(rebound_x, cast(rebound) * -out_velx, out_velx)\n",
    "out_vely = T.switch(rebound_y, cast(rebound) * -out_vely, out_vely)\n",
    "out_posx = T.switch(off_right, cast(1)-(out_posx-cast(1)), T.switch(off_left, -out_posx, out_posx))\n",
    "out_posy = T.switch(off_bott, cast(1)-(out_posy-cast(1)), T.switch(off_top, -out_posy, out_posy))\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "step = theano.function([], [],\n",
    "                       updates=[(_posx, out_posx), (_posy, out_posy),\n",
    "                                (_velx, out_velx), (_vely, out_vely)],\n",
    "                       allow_input_downcast=True,\n",
    "                       profile=True)\n",
    "render = theano.function([], [T.log(1+hist)], allow_input_downcast=True)\n",
    "reset = theano.function([], [], updates=[\n",
    "        (_posx, cuda_cast(_reset_posx())),\n",
    "        (_posy, cuda_cast(_reset_posy())),\n",
    "        (_velx, cuda_cast(_reset_velx())),\n",
    "        (_vely, cuda_cast(_reset_vely())),\n",
    "    ])\n",
    "\n",
    "theano.printing.debugprint(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scnerd\\Anaconda3\\envs\\Anaconda34\\lib\\site-packages\\ipykernel\\__main__.py:1: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  if __name__ == '__main__':\n",
      "  0%|                                                                                                                                                                                                                                             | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 4 1024 1024\n",
      "24 4 1024 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                                                                                                                                                                                  | 1/1000 [00:06<1:41:04,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 4 1024 1024\n",
      "24 4 1024 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▍                                                                                                                                                                                                                                  | 2/1000 [00:12<1:41:33,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 4 1024 1024\n",
      "24 4 1024 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▋                                                                                                                                                                                                                                  | 3/1000 [00:18<1:42:27,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 4 1024 1024\n",
      "24 4 1024 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▉                                                                                                                                                                                                                                  | 4/1000 [00:24<1:42:41,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 4 1024 1024\n",
      "24 4 1024 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|█▏                                                                                                                                                                                                                                 | 5/1000 [00:31<1:42:40,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 4 1024 1024\n",
      "24 4 1024 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█▎                                                                                                                                                                                                                                 | 6/1000 [00:37<1:42:16,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 4 1024 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function profiling\n",
      "==================\n",
      "  Message: <ipython-input-4-581141df6d6d>:61\n",
      "  Time in 6 calls to Function.__call__: 3.561615e+01s\n",
      "  Time in Function.fn.__call__: 3.561565e+01s (99.999%)\n",
      "  Time in thunks: 3.135791e+01s (88.044%)\n",
      "  Total compile time: 4.003683e+00s\n",
      "    Number of Apply nodes: 75\n",
      "    Theano Optimizer time: 5.460050e-01s\n",
      "       Theano validate time: 1.603484e-02s\n",
      "    Theano Linker time (includes C, CUDA code generation/compiling): 3.413587e+00s\n",
      "       Import time 3.556485e-01s\n",
      "\n",
      "Time in all call to theano.grad() 0.000000e+00s\n",
      "Time since theano import 92.625s\n",
      "Class\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>\n",
      "  33.3%    33.3%      10.453s       8.71e-01s     Py      12       2   theano.tensor.subtensor.AdvancedSubtensor\n",
      "  27.5%    60.8%       8.628s       1.80e-01s     C       48       8   theano.sandbox.cuda.basic_ops.GpuFromHost\n",
      "  24.0%    84.9%       7.540s       5.46e-02s     C      138      23   theano.tensor.elemwise.Elemwise\n",
      "  10.5%    95.4%       3.295s       9.15e-02s     C       36       6   theano.sandbox.cuda.basic_ops.HostFromGpu\n",
      "   4.0%    99.4%       1.247s       2.08e-01s     Py       6       1   __main__.PyCUDAHist2D\n",
      "   0.6%   100.0%       0.189s       3.50e-03s     C       54       9   theano.sandbox.cuda.basic_ops.GpuElemwise\n",
      "   0.0%   100.0%       0.003s       4.98e-04s     C        6       1   theano.sandbox.cuda.basic_ops.GpuAllocEmpty\n",
      "   0.0%   100.0%       0.002s       1.66e-04s     C       12       2   theano.sandbox.cuda.basic_ops.GpuReshape\n",
      "   0.0%   100.0%       0.001s       1.67e-04s     C        6       1   theano.sandbox.cuda.dnn.GpuDnnConv\n",
      "   0.0%   100.0%       0.001s       8.36e-05s     C        6       1   theano.tensor.basic.Join\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       24       4   theano.sandbox.cuda.basic_ops.GpuContiguous\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       24       4   theano.tensor.subtensor.Subtensor\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       24       4   theano.compile.ops.Shape_i\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       18       3   theano.sandbox.cuda.basic_ops.GpuDimShuffle\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       18       3   theano.tensor.opt.MakeVector\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       12       2   theano.sandbox.cuda.basic_ops.GpuSubtensor\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C        6       1   theano.sandbox.cuda.dnn.GpuDnnConvDesc\n",
      "   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Ops\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>\n",
      "  33.3%    33.3%      10.453s       8.71e-01s     Py      12        2   AdvancedSubtensor\n",
      "  27.5%    60.8%       8.628s       1.80e-01s     C       48        8   GpuFromHost\n",
      "  10.5%    71.4%       3.295s       9.15e-02s     C       36        6   HostFromGpu\n",
      "   9.9%    81.3%       3.107s       1.29e-01s     C       24        4   Elemwise{Cast{float32}}\n",
      "   5.3%    86.5%       1.650s       1.37e-01s     C       12        2   Elemwise{Cast{int32}}\n",
      "   4.1%    90.7%       1.294s       1.08e-01s     C       12        2   Elemwise{Composite{Cast{float32}(OR(i0, i1))}}\n",
      "   4.0%    94.6%       1.247s       2.08e-01s     Py       6        1   PyCUDAHist2D{xres=320, yres=240, xmin=0, xptp=1, ymin=0, yptp=1, length=1024, num_blocks=4, threads=1024}\n",
      "   2.7%    97.3%       0.831s       6.93e-02s     C       12        2   Elemwise{gt,no_inplace}\n",
      "   2.1%    99.4%       0.658s       5.48e-02s     C       12        2   Elemwise{lt,no_inplace}\n",
      "   0.6%   100.0%       0.188s       3.13e-02s     C        6        1   GpuElemwise{sqr,no_inplace}\n",
      "   0.0%   100.0%       0.003s       4.98e-04s     C        6        1   GpuAllocEmpty\n",
      "   0.0%   100.0%       0.002s       3.33e-04s     C        6        1   GpuReshape{3}\n",
      "   0.0%   100.0%       0.002s       2.51e-04s     C        6        1   GpuElemwise{mul,no_inplace}\n",
      "   0.0%   100.0%       0.001s       1.67e-04s     C        6        1   GpuDnnConv{algo='small', inplace=True}\n",
      "   0.0%   100.0%       0.001s       8.36e-05s     C        6        1   Join\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       24        4   GpuContiguous\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       24        4   Subtensor{int64}\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       18        3   MakeVector{dtype='int64'}\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       12        2   Shape_i{0}\n",
      "   0.0%   100.0%       0.000s       0.00e+00s     C       12        2   Elemwise{Composite{((((i0 + i1) - i2) // i3) + i3)}}[(0, 0)]\n",
      "   ... (remaining 19 Ops account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Apply\n",
      "------\n",
      "<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>\n",
      "  16.7%    16.7%       5.244s       8.74e-01s      6    70   AdvancedSubtensor(HostFromGpu.0, Elemwise{Cast{int32}}.0, Elemwise{Cast{int32}}.0)\n",
      "  16.6%    33.3%       5.210s       8.68e-01s      6    69   AdvancedSubtensor(HostFromGpu.0, Elemwise{Cast{int32}}.0, Elemwise{Cast{int32}}.0)\n",
      "   6.1%    39.4%       1.902s       3.17e-01s      6    41   GpuFromHost(Elemwise{Composite{Cast{float32}(OR(i0, i1))}}.0)\n",
      "   4.7%    44.1%       1.467s       2.44e-01s      6    38   GpuFromHost(Elemwise{Composite{Cast{float32}(OR(i0, i1))}}.0)\n",
      "   4.5%    48.6%       1.423s       2.37e-01s      6    72   GpuFromHost(AdvancedSubtensor.0)\n",
      "   4.3%    52.9%       1.354s       2.26e-01s      6    37   GpuFromHost(Elemwise{Cast{float32}}.0)\n",
      "   4.0%    56.9%       1.247s       2.08e-01s      6     5   PyCUDAHist2D{xres=320, yres=240, xmin=0, xptp=1, ymin=0, yptp=1, length=1024, num_blocks=4, threads=1024}(GpuContiguous.0, GpuContiguous.0, GpuContiguous.0)\n",
      "   3.1%    60.0%       0.957s       1.59e-01s      6    26   Elemwise{Cast{float32}}(Elemwise{gt,no_inplace}.0)\n",
      "   3.0%    63.0%       0.944s       1.57e-01s      6    16   Elemwise{Cast{int32}}(HostFromGpu.0)\n",
      "   3.0%    66.0%       0.940s       1.57e-01s      6    29   Elemwise{Cast{float32}}(Elemwise{gt,no_inplace}.0)\n",
      "   3.0%    69.0%       0.939s       1.57e-01s      6    15   HostFromGpu(GpuElemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)].0)\n",
      "   2.6%    71.6%       0.820s       1.37e-01s      6    13   HostFromGpu(GpuElemwise{Composite{(i0 + (i1 * i2))}}[(0, 0)].0)\n",
      "   2.5%    74.0%       0.772s       1.29e-01s      6     9   HostFromGpu(PyCUDAHist2D{xres=320, yres=240, xmin=0, xptp=1, ymin=0, yptp=1, length=1024, num_blocks=4, threads=1024}.1)\n",
      "   2.4%    76.5%       0.758s       1.26e-01s      6    10   HostFromGpu(PyCUDAHist2D{xres=320, yres=240, xmin=0, xptp=1, ymin=0, yptp=1, length=1024, num_blocks=4, threads=1024}.2)\n",
      "   2.3%    78.7%       0.706s       1.18e-01s      6    17   Elemwise{Cast{int32}}(HostFromGpu.0)\n",
      "   2.1%    80.8%       0.670s       1.12e-01s      6    27   Elemwise{Composite{Cast{float32}(OR(i0, i1))}}(Elemwise{lt,no_inplace}.0, Elemwise{gt,no_inplace}.0)\n",
      "   2.0%    82.9%       0.638s       1.06e-01s      6    42   GpuFromHost(Elemwise{Cast{float32}}.0)\n",
      "   2.0%    84.9%       0.624s       1.04e-01s      6    30   Elemwise{Composite{Cast{float32}(OR(i0, i1))}}(Elemwise{lt,no_inplace}.0, Elemwise{gt,no_inplace}.0)\n",
      "   2.0%    86.9%       0.623s       1.04e-01s      6    40   GpuFromHost(Elemwise{Cast{float32}}.0)\n",
      "   2.0%    88.8%       0.619s       1.03e-01s      6    39   GpuFromHost(Elemwise{Cast{float32}}.0)\n",
      "   ... (remaining 55 Apply instances account for 11.17%(3.50s) of the runtime)\n",
      "\n",
      "Here are tips to potentially make your code run faster\n",
      "                 (if you think of new ones, suggest them on the mailing list).\n",
      "                 Test them first, as they are not guaranteed to always provide a speedup.\n",
      "  - Try the Theano flag floatX=float32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ec54b80fedfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mrenders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\scnerd\\Anaconda3\\envs\\Anaconda34\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\scnerd\\Anaconda3\\envs\\Anaconda34\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lump = np.random.choice(np.arange(n), 0.1 * n, replace=False)\n",
    "\n",
    "reset()\n",
    "\n",
    "from tqdm import tqdm\n",
    "print_every_iter = False\n",
    "renders = []\n",
    "try:\n",
    "    for i in tqdm(range(1000)):\n",
    "        step()\n",
    "        r, = render()\n",
    "        renders.append(np.array(r))\n",
    "finally:\n",
    "    step.profile.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "renders = [np.array(r[0]).sum(axis=0).sum(axis=-1) for r in __renders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "for i, rendered in enumerate(tqdm(renders)):\n",
    "    plt.imsave('data/wave_render_5/frame_%d.png' % i, rendered, cmap=plt.cm.inferno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
